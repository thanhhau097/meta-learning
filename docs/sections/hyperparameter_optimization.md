# hyperparameter_optimization

Hyperparameter Optimization (HO) is within the remit of meta-learning, in that hyperparameters such as learning rate
 or regularization strength can be included in the definition of ‘how to learn’. Here we focus on HO tasks defining 
 a meta objective that is trained end-to-end with neural networks. This includes some work in HO, 
 such as gradient-based hyperparameter learning [67] and neural architecture search [26]. 
 But we exclude other approaches like random search [69] and Bayesian Hyperparameter Optimization [70], which are rarely considered to be meta- learning.

<!-- REFERENCE -->


<details>
<summary>[69] Random Search For Hyper-Parameter Optimization</summary>
<br>
<!-- (random_search_for_hyper_parameter_optimization.md) -->

# random_search_for_hyper_parameter_optimization.md

<!-- REFERENCE -->


[Random Search For Hyper-Parameter Optimization](../papers/random_search_for_hyper_parameter_optimization.md)

</details>



<details>
<summary>[70] Taking The Human Out Of The Loop: A Review Of Bayesian Optimization</summary>
<br>
<!-- (taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization.md) -->

# taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization.md

<!-- REFERENCE -->


[Taking The Human Out Of The Loop: A Review Of Bayesian Optimization](../papers/taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization.md)

</details>



<details>
<summary>[26] DARTS: Differentiable Architecture Search</summary>
<br>
<!-- (darts_differentiable_architecture_search.md) -->

# darts_differentiable_architecture_search.md

<!-- REFERENCE -->


[DARTS: Differentiable Architecture Search](../papers/darts_differentiable_architecture_search.md)

</details>



<details>
<summary>[67] Forward And Reverse Gradient-Based Hyperparameter Optimization</summary>
<br>
<!-- (forward_and_reverse_gradient_based_hyperparameter_optimization.md) -->

# forward_and_reverse_gradient_based_hyperparameter_optimization.md

<!-- REFERENCE -->


[Forward And Reverse Gradient-Based Hyperparameter Optimization](../papers/forward_and_reverse_gradient_based_hyperparameter_optimization.md)

</details>

