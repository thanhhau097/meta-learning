# bayesian_meta_learning
Bayesian meta-learning approaches formalize meta-learning via Bayesian hierarchical modelling, and use Bayesian infer- ence for learning rather than direct optimization of parame- ters. In the meta-learning context, Bayesian learning is typ- ically intractable, and so different approximation methods can be used. Variational approaches, especially stochastic variational methods, are the most common, but sampling approaches can also be considered. One by-product of Bayesian meta-learning is that it
provides uncertainty measures for the θ parameters, and hence measures of prediction uncertainty. Knowing the uncertainty of learner’s predictions can be vital in safety critical domains such as few-shot medical tasks, and can be used for exploration in Reinforcement Learning and for some active learning methods, where a model can seek information about datapoints with high uncertainty.

Recently a number of authors have explored Bayesian
approaches to meta-learning complex models with com- petitive results. Many of these have utilized deep neural networks as components within the framework, for ex- ample extending variational autoencoders to model task variables explicitly [71]. Neural Processes [166] aim to com- bine the uncertainty quantification of Gaussian Processes with the versatility of neural networks, but they are not shown to work on modern few-shot benchmarks. Deep kernel learning is also an active research area that has been adapted to the meta-learning setting [243], and is often coupled with Gaussian Processes [213]. In [72] gradient based meta-learning is recast into a hierarchical empirical Bayes inference problem (i.e. prior learning), which models uncertainty in task-specific parameters θ. Bayesian MAML [212] improves on this model by using a Bayesian ensemble approach that allows non-Gaussian posteriors over θ, and later work removes the need for costly ensembles [199], [244]. In Probabilistic MAML [95], it is the uncertainty in the metaknowledge ω that is modelled, while a MAP estimate is used for θ. Increasingly, these Bayesian methods are shown to tackle ambiguous tasks, active learning and RL problems. Separate from the above approaches, meta-learning has
also been proposed to aid the Bayesian inference process itself. By way of example, in [245], the authors use a meta- learning framework to adapt a Bayesian sampler to provide efficient adaptive sampling methods.

Benchmarks In Bayesian meta-learning, the point is usu- ally to model the uncertainty in the predictions of our meta- learner, and so performance on standard few-shot classifica- tion benchmarks doesn’t necessarily capture what we care about. For this reason different tasks have been developed in the literature. Bayesian MAML [212] extends the sinusoidal regression task of MAML [19] to make it more challenging. Probabilistic MAML [95] provides a suite of 1D toy exam- ples capable of showing model uncertainty and how this uncertainty can be used in an active learning scenario. It also creates a binary classification task from celebA [246], where the positive class is determined by the presence of two facial attributes, but training images show three attributes, thus introducing ambiguity in which two attributes should be classified on. It is observed that sampling ω can correctly reflect this ambiguity. Active learning toy experiments are also shown in [212] as well as reinforcement learning ap- plications, and ambiguous one-shot image generation tasks are used in [199]. Finally, some researchers propose to look at the accuracy v.s. confidence of the meta-learners (i.e. their calibration) [244].


<!-- REFERENCE -->
