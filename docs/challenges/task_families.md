# task_families

Many existing meta-learning frameworks, especially for few-shot learning, require task families for
meta-training. While this indeed reflects lifelong human learning, in some applications data for such task families may not be available. How to relax this assumption is an ongoing challenge. Unsupervised meta-learning [247]â€“[249] and online meta-learning methods [44], [162], [167], [168], [185], could help to alleviate this; as can improvements in meta-generalization discussed above.
<!-- REFERENCE -->


<details>
<summary>[162] Meta-Gradient Reinforcement Learning</summary>
<br>
<!-- (meta_gradient_reinforcement_learning.md) -->

# meta_gradient_reinforcement_learning.md

<!-- REFERENCE -->


[Meta-Gradient Reinforcement Learning](../papers/meta_gradient_reinforcement_learning.md)

</details>



<details>
<summary>[167] Discovery Of Useful Questions As Auxiliary Tasks</summary>
<br>
<!-- (discovery_of_useful_questions_as_auxiliary_tasks.md) -->

# discovery_of_useful_questions_as_auxiliary_tasks.md

<!-- REFERENCE -->


[Discovery Of Useful Questions As Auxiliary Tasks](../papers/discovery_of_useful_questions_as_auxiliary_tasks.md)

</details>



<details>
<summary>[44] Feature-Critic Networks For Heterogeneous Domain Generalization</summary>
<br>
<!-- (feature_critic_networks_for_heterogeneous_domain_generalization.md) -->

# feature_critic_networks_for_heterogeneous_domain_generalization.md

<!-- REFERENCE -->


[Feature-Critic Networks For Heterogeneous Domain Generalization](../papers/feature_critic_networks_for_heterogeneous_domain_generalization.md)

</details>



<details>
<summary>[185] Online Learning Of A Memory For Learning Rates</summary>
<br>
<!-- (online_learning_of_a_memory_for_learning_rates.md) -->

# online_learning_of_a_memory_for_learning_rates.md

<!-- REFERENCE -->


[Online Learning Of A Memory For Learning Rates](../papers/online_learning_of_a_memory_for_learning_rates.md)

</details>



<details>
<summary>[249] Assume, Augment And Learn: Unsupervised Few-shot Meta-learning Via Random Labels And Data Augmentation</summary>
<br>
<!-- (assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md) -->

# assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md

<!-- REFERENCE -->


[Assume, Augment And Learn: Unsupervised Few-shot Meta-learning Via Random Labels And Data Augmentation](../papers/assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md)

</details>



<details>
<summary>[168] On Learning Intrinsic Rewards For Policy Gradient Methods</summary>
<br>
<!-- (on_learning_intrinsic_rewards_for_policy_gradient_methods.md) -->

# on_learning_intrinsic_rewards_for_policy_gradient_methods.md

<!-- REFERENCE -->


[On Learning Intrinsic Rewards For Policy Gradient Methods](../papers/on_learning_intrinsic_rewards_for_policy_gradient_methods.md)

</details>



<details>
<summary>[247] Unsupervised Learning Via Meta-learning</summary>
<br>
<!-- (unsupervised_learning_via_meta_learning.md) -->

# unsupervised_learning_via_meta_learning.md

<!-- REFERENCE -->


[Unsupervised Learning Via Meta-learning](../papers/unsupervised_learning_via_meta_learning.md)

</details>

