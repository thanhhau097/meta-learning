# multi_vs_single_task
When the goal is to tune the learner to better solve any task drawn from a given family, then inner loop learning episodes correspond to a randomly drawn task from p(T) [19], [20], [44]. When the goal is to tune the learner to simply solve one specific task better, then
the inner loop learning episodes all draw data from the same underlying task [67], [78], [162], [167], [168], [185]. It is worth noting that these two meta-objectives tend
to have different assumptions and value propositions. The multi-task objective obviously requires a task family p(T) to work with, which single-task does not. Meanwhile for multi-task, the data and compute cost of meta-training can be amortized by potentially boosting the performance of multiple target tasks during meta-test; but single-task – without the new tasks for amortization – needs to improve the final solution or asymptotic performance of the current task, or meta-learn fast enough to be online.
<!-- REFERENCE -->


<details>
<summary>[162] Meta-Gradient Reinforcement Learning</summary>
<br>
<!-- (meta_gradient_reinforcement_learning.md) -->

# meta_gradient_reinforcement_learning.md

<!-- REFERENCE -->


[Meta-Gradient Reinforcement Learning](../papers/meta_gradient_reinforcement_learning.md)

</details>



<details>
<summary>[167] Discovery Of Useful Questions As Auxiliary Tasks</summary>
<br>
<!-- (discovery_of_useful_questions_as_auxiliary_tasks.md) -->

# discovery_of_useful_questions_as_auxiliary_tasks.md

<!-- REFERENCE -->


[Discovery Of Useful Questions As Auxiliary Tasks](../papers/discovery_of_useful_questions_as_auxiliary_tasks.md)

</details>



<details>
<summary>[44] Feature-Critic Networks For Heterogeneous Domain Generalization</summary>
<br>
<!-- (feature_critic_networks_for_heterogeneous_domain_generalization.md) -->

# feature_critic_networks_for_heterogeneous_domain_generalization.md

<!-- REFERENCE -->


[Feature-Critic Networks For Heterogeneous Domain Generalization](../papers/feature_critic_networks_for_heterogeneous_domain_generalization.md)

</details>



<details>
<summary>[20] Prototypical Networks For Few Shot Learning</summary>
<br>
<!-- (prototypical_networks_for_few_shot_learning.md) -->

# prototypical_networks_for_few_shot_learning.md

<!-- REFERENCE -->


[Prototypical Networks For Few Shot Learning](../papers/prototypical_networks_for_few_shot_learning.md)

</details>



<details>
<summary>[67] Forward And Reverse Gradient-Based Hyperparameter Optimization</summary>
<br>
<!-- (forward_and_reverse_gradient_based_hyperparameter_optimization.md) -->

# forward_and_reverse_gradient_based_hyperparameter_optimization.md

<!-- REFERENCE -->


[Forward And Reverse Gradient-Based Hyperparameter Optimization](../papers/forward_and_reverse_gradient_based_hyperparameter_optimization.md)

</details>



<details>
<summary>[185] Online Learning Of A Memory For Learning Rates</summary>
<br>
<!-- (online_learning_of_a_memory_for_learning_rates.md) -->

# online_learning_of_a_memory_for_learning_rates.md

<!-- REFERENCE -->


[Online Learning Of A Memory For Learning Rates](../papers/online_learning_of_a_memory_for_learning_rates.md)

</details>



<details>
<summary>[19] Model-Agnostic Meta-learning For Fast Adaptation Of Deep Networks</summary>
<br>
<!-- (model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md) -->

# model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md

<!-- REFERENCE -->


[Model-Agnostic Meta-learning For Fast Adaptation Of Deep Networks](../papers/model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md)

</details>



<details>
<summary>[168] On Learning Intrinsic Rewards For Policy Gradient Methods</summary>
<br>
<!-- (on_learning_intrinsic_rewards_for_policy_gradient_methods.md) -->

# on_learning_intrinsic_rewards_for_policy_gradient_methods.md

<!-- REFERENCE -->


[On Learning Intrinsic Rewards For Policy Gradient Methods](../papers/on_learning_intrinsic_rewards_for_policy_gradient_methods.md)

</details>



<details>
<summary>[78] Learning To Learn By Gradient Descent By Gradient Descent</summary>
<br>
<!-- (learning_to_learn_by_gradient_descent_by_gradient_descent.md) -->

# learning_to_learn_by_gradient_descent_by_gradient_descent.md

<!-- REFERENCE -->


[Learning To Learn By Gradient Descent By Gradient Descent](../papers/learning_to_learn_by_gradient_descent_by_gradient_descent.md)

</details>

