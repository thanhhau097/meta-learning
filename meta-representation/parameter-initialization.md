# Parameter Initialization 

In this first family of methods $ \alpha $ corresponds to the initial parameters of a neural network. In MAML [19], [95], [96] these are interpreted as initial conditions of the inner optimization. A good initialization is just a few gradient steps away from a solution to any task T drawn from p(T). These approaches are widely used for few-shot learning, where target problems can be learned without over-fitting using few examples, given such a carefully chosen initial condition. A key challenge with this approach is that the outer optimization needs to solve for as many parameters as the inner optimization (poten- tially hundreds of millions in large CNNs). This leads to a line of work on isolating a subset of parameters to meta- learn. For example by subspace [74], [97], by layer [79], [97], [98], or by separating out scale and shift [99]. While inner loop initialization is a popular and effective choice of meta-representation, a key debate here is whether a single initial condition is sufficient to provide fast learning for a wide range of potential tasks, or if one is limited to fairly narrow distributions p(T). This has led to variants that model mixtures over multiple initial conditions [97], [100], [101].

### References