# discussion_transductive_representations_and_methods
Most of the representations ω discussed above are param- eter vectors of functions that process or generate data.
However a few of the representations mentioned are trans- ductive in the sense that the ω literally corresponds to data points [144], labels [146], or per-sample weights [142]. This means that the number of parameters in ω to meta-learn scales as the size of the dataset. While the success of these methods is a testament to the capabilities of contemporary meta-learning [145], this property may ultimately limit their scalability. Distinct from a transductive representation are methods
that are transductive in the sense that they are designed to operate on the query instances as well as support instances [98], [122].

<!-- REFERENCE -->


<details>
<summary>[122] Self-supervised Generalisation With Meta Auxiliary Learning</summary>
<br>
<!-- (self_supervised_generalisation_with_meta_auxiliary_learning.md) -->

# self_supervised_generalisation_with_meta_auxiliary_learning.md

<!-- REFERENCE -->


[Self-supervised Generalisation With Meta Auxiliary Learning](../papers/self_supervised_generalisation_with_meta_auxiliary_learning.md)

</details>



<details>
<summary>[145] Optimizing Millions Of Hyperparameters By Implicit Differentiation</summary>
<br>
<!-- (optimizing_millions_of_hyperparameters_by_implicit_differentiation.md) -->

# optimizing_millions_of_hyperparameters_by_implicit_differentiation.md

<!-- REFERENCE -->


[Optimizing Millions Of Hyperparameters By Implicit Differentiation](../papers/optimizing_millions_of_hyperparameters_by_implicit_differentiation.md)

</details>



<details>
<summary>[98] Learning To Learn By SelfCritique</summary>
<br>
<!-- (learning_to_learn_by_selfcritique.md) -->

# learning_to_learn_by_selfcritique.md

<!-- REFERENCE -->


[Learning To Learn By SelfCritique](../papers/learning_to_learn_by_selfcritique.md)

</details>



<details>
<summary>[144] Dataset Distillation</summary>
<br>
<!-- (dataset_distillation.md) -->

# dataset_distillation.md

<!-- REFERENCE -->


[Dataset Distillation](../papers/dataset_distillation.md)

</details>



<details>
<summary>[142] Meta-Weight-Net: Learning An Explicit Mapping For Sample Weighting</summary>
<br>
<!-- (meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md) -->

# meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md

<!-- REFERENCE -->


[Meta-Weight-Net: Learning An Explicit Mapping For Sample Weighting](../papers/meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md)

</details>



<details>
<summary>[146] Learning To Impute: A General Framework For Semi-supervised Learning</summary>
<br>
<!-- (learning_to_impute_a_general_framework_for_semi_supervised_learning.md) -->

# learning_to_impute_a_general_framework_for_semi_supervised_learning.md

<!-- REFERENCE -->


[Learning To Impute: A General Framework For Semi-supervised Learning](../papers/learning_to_impute_a_general_framework_for_semi_supervised_learning.md)

</details>

