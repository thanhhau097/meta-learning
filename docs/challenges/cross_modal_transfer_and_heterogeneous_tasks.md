# cross_modal_transfer_and_heterogeneous_tasks

Most meta-learning methods studied so far have considered tasks all drawn from the same modality such as vision, text, proprioceptive state, or audio. Humans appear to be able to transfer knowledge across modalities (e.g., by visual imitation learning). How to do meta-learning that extracts abstract knowledge from a set of tasks that may each span a unique modality is an open question. Most studies have addressed transfer between tasks of the same type such as object recognition, but ideally we would like to be able to transfer between heterogeneous tasks such as those studied in Taskonomy [289].
<!-- REFERENCE -->


<details>
<summary>[289] Taskonomy: Disentangling Task Transfer Learning</summary>
<br>
<!-- (taskonomy_disentangling_task_transfer_learning.md) -->

# taskonomy_disentangling_task_transfer_learning.md

<!-- REFERENCE -->


[Taskonomy: Disentangling Task Transfer Learning](../papers/taskonomy_disentangling_task_transfer_learning.md)

</details>

