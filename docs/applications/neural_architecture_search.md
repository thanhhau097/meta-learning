# neural_architecture_search
Architecture search [26]–[28], [39], [123] can be seen as cor- responding to a kind hyperparameter optimization where ω specifies the architecture of a neural network. The inner optimization trains networks with the specified architecture, and the outer optimization searches for architectures with good validation performance. NAS methods are commonly analysed [39] according to ‘search space’, ‘search strategy’, and ‘performance estimation strategy. These correspond to the hypothesis space for ω, the meta-optimization strategy, and the meta-objective. NAS is particularly challenging because: (i) Fully evaluating the inner loop is generally very expensive since it requires training a many-shot neural network to completion. This leads to approximations such
as sub-sampling the train set, early termination of the inner loop, and ultimately approximations such as interleaved descent on both ω and θ [26] as in online meta-learning. (ii.) The search space is hard to define, and optimizing it is costly. This is because most search spaces are broad, and represent architectures that aren’t differentiable. This leads to methods that perform cell-level search [26], [28] to constrain the search space; and then rely on RL [28], discrete gradient estimators that provide a differentiable approximation to the search space [26], [124], and evolution [27], [123].

Examples Some notable examples include: (i) NASNet [28], [234] where the search space is restricted to cell-level learning, and defined as a string generated by an RNN which indicates what operations should be at what parts of the cell-tree, optimized using RL. (ii) Reqularized Evolution [27] where the authors use NASNet’s search space but optimize it using regularized evolution, i.e. standard tour- nament based evolution with removal of oldest individuals after every iteration. (iii.) DARTS [26] where the authors carefully cast the space of cell architectures as a sequence of softmax selections over a number of pre-selected operations, thus making the search space differentiable. Learning the architecture then corresponds to jointly learning the softmax weights with the network parameters. This allows architec- ture learning to be sped up by 2-3 levels of magnitude both in computational overheads and wall-clock time. (iv) T-NAS [125] where the authors utilize the DARTS search space, but train it using a data-flow that enforces the architecture to be learned using very few data-points and very few updates, while keeping the generalization performance high. As a result of learning such softmax weights, they achieve few- shot architecture search. Once trained, these weights can be adapted to new tasks within seconds rather than days.

An interesting special case of NAS is activation func-
tion search [151]. While hand-designed activation functions such as ReLU are dominant in NN literature, a successful example of NAS meta-learning is the discovery of the Swish activation function [151] with RL in the space of symbolic activation functions. Swish has gone on to contribute to several influential state-of-the-art and general purpose CNN architectures [235], [236]

**Multi-Objective NAS** Architectures to be deployed on mobile devices have additional constraints besides valida- tion accuracy [7], and NAS can also be deployed to produce compact and efficient models [237]. This can be realised by defining a multi-objective meta-objective that contains terms related both to validation performance as well as latency or size of the model product by a given θ, and thus leading to good performance-cost tradeoffs.

**Topical Issues** While NAS itself can be seen as an instance of hyper-parameter or hypothesis-class meta-learning, it can also interact with meta-learning in other forms. Since NAS is costly, a topical issue is whether discovered architectures are dataset specific, or general purpose with ability to generalize to new problems [234]. Recent results suggest that meta- training across multiple datasets can lead to improved cross- task generalization of architectures [126]. While few-shot meta-learning is typically addressed
from a parameter learning perspective in the context of hand-crafted architectures [19], [20], [87], one can also define NAS meta-objectives to train an architecture suitable for few-shot learning [238], [239]. Furthermore, analogously to fast-adapting initial condition meta-learning approaches such as MAML [19], one can train good initial architectures [125] or architecture priors [126] that are easy to adapt towards specific tasks.

**Benchmarks NAS** is often evaluated on the CIFAR-10 dataset. However even on this small dataset, architecture search is costly to perform, making it inaccessible to many researchers; and furthermore results are hard to reproduce due to other confounding factors such as tuning of hy- perparameters [240]. To support reproducible and accessi- ble research, the recently released NASbenches [241], [242] provides pre-computed performance measures for a large number of network architectures.
<!-- REFERENCE -->
