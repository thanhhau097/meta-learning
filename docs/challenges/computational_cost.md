# computational_cost
Naive implementation of bilevel op- timization as shown in Section 2.1 leads to a quadratic number of learning steps, since each outer step requires multiple inner steps. Moreover, there are a large number of inner steps in the case of many-shot experiments, and these need to be stored in memory. For this reason most meta-learning frameworks are extremely expensive in both time and memory, and often limited to small architectures in the few-shot regime [19]. However there is an increasing focus on methods to tackle this problem. For instance, one can alternate inner and outer updates [44], or train surrogate models [108]. Another family of recent approaches acceler- ate meta-training via closed-form solvers in the inner loop [152], [154]. However, the cost is still quite large, and the significance of the former set heuristics for convergence is unclear. A recent method for computing the gradients for the outer loop using implicit gradients provides a cheaper alternative [153], but only focused on learning the initializa- tion of a network for MAML. While implicit gradients were then shown to work for more general meta-learning tasks such as learning an augmentation network [145], they can only learn parameters involved in the loss function directly and make several assumptions (like zero training gradients at θ∗) often leading to inaccurate ω gradients.
<!-- REFERENCE -->
