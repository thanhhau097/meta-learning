# unsupervised_meta_learning_and_meta_learning_unsupervised_learning

In the meta-learning literature, there exist two main variants of meta-learning involving unsupervised learning. In the first one the meta-objective of the outer loop is unsuper- vised, and therefore the learner itself is learned without any labels available. We refer to this case as Unsupervised Meta-Learning. In the second variant, meta-learning is used as a means to learn an unsupervised inner loop task. The outer objective in this case can be anything from supervised, unsupervised or reinforcement based. We refer to this as Meta-Learning Unsupervised Learning. Unsupervised Meta-Learning [247]â€“[249] aims to relax the conventional assumption of an annotated set of source tasks for meta-training, while still producing good downstream performance for supervised few-shot learning. Typically synthetic source tasks are constructed without supervision via clustering or class-preserving data augmentation. Meta-Learning Unsupervised Learning aims to use meta-
learning to train unsupervised learning algorithms that work well for downstream supervised learning tasks. One can train unsupervised clustering algorithms [21], [250], [251] or losses [98], [116] such that downstream supervised learning performance is optimized. This helps to deal with the ill-definedness of the unsupervised learning problem by transforming it into a problem with a clear (meta) super- vised objective.
<!-- REFERENCE -->


<details>
<summary>[249] Assume, Augment And Learn: Unsupervised Few-shot Meta-learning Via Random Labels And Data Augmentation</summary>
<br>
<!-- (assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md) -->

# assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md

<!-- REFERENCE -->


[Assume, Augment And Learn: Unsupervised Few-shot Meta-learning Via Random Labels And Data Augmentation](../papers/assume_augment_and_learn_unsupervised_few_shot_meta_learning_via_random_labels_and_data_augmentation.md)

</details>



<details>
<summary>[251] Meta-Learning To Cluster</summary>
<br>
<!-- (meta_learning_to_cluster.md) -->

# meta_learning_to_cluster.md

<!-- REFERENCE -->


[Meta-Learning To Cluster](../papers/meta_learning_to_cluster.md)

</details>



<details>
<summary>[250] Supervising Unsupervised Learning</summary>
<br>
<!-- (supervising_unsupervised_learning.md) -->

# supervising_unsupervised_learning.md

<!-- REFERENCE -->


[Supervising Unsupervised Learning](../papers/supervising_unsupervised_learning.md)

</details>



<details>
<summary>[98] Learning To Learn By SelfCritique</summary>
<br>
<!-- (learning_to_learn_by_selfcritique.md) -->

# learning_to_learn_by_selfcritique.md

<!-- REFERENCE -->


[Learning To Learn By SelfCritique](../papers/learning_to_learn_by_selfcritique.md)

</details>



<details>
<summary>[247] Unsupervised Learning Via Meta-learning</summary>
<br>
<!-- (unsupervised_learning_via_meta_learning.md) -->

# unsupervised_learning_via_meta_learning.md

<!-- REFERENCE -->


[Unsupervised Learning Via Meta-learning](../papers/unsupervised_learning_via_meta_learning.md)

</details>



<details>
<summary>[116] Semi-Supervised Few-Shot Learning With MAML</summary>
<br>
<!-- (semi_supervised_few_shot_learning_with_maml.md) -->

# semi_supervised_few_shot_learning_with_maml.md

<!-- REFERENCE -->


[Semi-Supervised Few-Shot Learning With MAML](../papers/semi_supervised_few_shot_learning_with_maml.md)

</details>



<details>
<summary>[21] Meta-learning Update Rules For Unsupervised Representation Learning</summary>
<br>
<!-- (meta_learning_update_rules_for_unsupervised_representation_learning.md) -->

# meta_learning_update_rules_for_unsupervised_representation_learning.md

<!-- REFERENCE -->


[Meta-learning Update Rules For Unsupervised Representation Learning](../papers/meta_learning_update_rules_for_unsupervised_representation_learning.md)

</details>

