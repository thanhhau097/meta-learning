# attention_modules

Attention mechanisms have been shown to improve generalization performance and inter- pretability. Such mechanisms have also formed part of the meta-representation of various meta-learning models. For example, they have been used as comparators of support and target set items for metric-based transductive meta- learners [127], as well as feature extractors to prevent catastrophic forgetting in few-shot continual learning [128]. More recently, attention was also used to summarize the distribution of an incoming text classification task [129].
<!-- REFERENCE -->


<details>
<summary>[127] Cross Attention Network For Few-shot Classification</summary>
<br>
<!-- (cross_attention_network_for_few_shot_classification.md) -->

# cross_attention_network_for_few_shot_classification.md

<!-- REFERENCE -->


[Cross Attention Network For Few-shot Classification](../papers/cross_attention_network_for_few_shot_classification.md)

</details>



<details>
<summary>[129] Few-shot Text Classification With Distributional Signatures</summary>
<br>
<!-- (few_shot_text_classification_with_distributional_signatures.md) -->

# few_shot_text_classification_with_distributional_signatures.md

<!-- REFERENCE -->


[Few-shot Text Classification With Distributional Signatures](../papers/few_shot_text_classification_with_distributional_signatures.md)

</details>



<details>
<summary>[128] Incremental Few-shot Learning With Attention Attractor Networks</summary>
<br>
<!-- (incremental_few_shot_learning_with_attention_attractor_networks.md) -->

# incremental_few_shot_learning_with_attention_attractor_networks.md

<!-- REFERENCE -->


[Incremental Few-shot Learning With Attention Attractor Networks](../papers/incremental_few_shot_learning_with_attention_attractor_networks.md)

</details>

