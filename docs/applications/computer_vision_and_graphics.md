# computer_vision_and_graphics

In this section we discuss the ways in which meta-learning has been exploited – in terms of application domains such as computer vision and reinforcement learning and cross-cutting problems such as architecture search, hyper- parameter optimization, Bayesian and unsupervised meta- learning.

**5.1 Computer Vision and Graphics Computer vision** is one of the major consumer domains of meta-learning techniques. This is particularly driven by the impact of meta-learning on few-shot learning which holds promise to deal with the challenge posed by the long-tail of concepts to recognise in vision.

**5.1.1 Few-Shot Learning Methods Few-shot learning (FSL)** is extremely challenging, especially for large neural network models [1], [15], where data vol- ume is often the dominant factor in performance [188], and training large models with small datasets leads to over- fitting or even non-convergence. Meta-learning-based few- shot learning methods train algorithms that enable powerful deep networks to successfully learn on small datasets. There are numerous vision problems where meta-learning helps

in the few-shot setting, and we provide a non-exhaustive summary as follows. 

**Classification **The most common application of meta- learning thus far is few-shot multi-class classification in image recognition, where the inner and outer loss functions are typically the cross entropy over training and validation data respectively [19], [20], [41], [73], [75], [76], [86], [88], [97], [98], [101], [161], [189]–[193]. Optimizer-centric [19], black-box [40], [79] and metric learning [86]–[88] models have all been considered. Relevant benchmarks are covered in Section 5.1.2. This line of work has led to a steady improvement in performance compared to early methods [19], [85], [86]. However, performance is still far behind that of fully super- vised methods, so there is more work to be done. Current research issues include few-shot models with better cross- domain generalization [173], recognition within the joint label space defined by meta-train and meta-test classes [80], and incremental addition of new few-shot classes [128], [165].

**Object Detection** Building on the rapid progress in few- shot classification, recent work has also generalized to few- shot object detection [165], [194], often using feed-forward hypernetwork-based approaches to embed support set im- ages and synthesize final layer classification weights in the base model.

**Landmark Prediction** The goal of landmark estimation is to find the location of skeleton key points within an image, such as such as joints in human or robot images. This is typically formulated as an image-conditional regression problem. For example, a MAML-based model was shown to work for human pose estimation [195], modular-meta- learning was successfully applied to robotics [130], while a hypernetwork-based model was applied to few-shot clothes fitting for novel fashion items [165].

**Object Segmentation** Few-shot object segmentation is important due to the cost of obtaining pixel-wise labeled images in this domain. Meta-learning methods based on hypernetworks have been shown to work in the one-shot regime [196], and performance was later improved by adapting prototypical networks [197]. Other models tackle cases where segmentation has low density [198].

**Image Generation** In [199] an amortized probabilistic meta-learner is used to generate multiple views of an object from just a single image, and talking faces are generated from little data by learning the initialization of an adversar- ial model for quick adaptation [200].

**Video Synthesis** In [201], the authors propose meta-learning a weight generator that takes as input a few frames and generates a network that can achieve strong results in video synthesis for the given task.

**Density Estimation** Since autoregressive models typically require large depths to capture the distribution of the data, the few-shot regime lands itself to overfitting and is partic- ularly challenging. Meta-learning coupled with an attention mechanism has shown to enable PixelCNNs to shine in such a regime [202].

**5.1.2 Few-Shot Learning Benchmarks** Progress in AI and machine learning is often measured by, and spurred by, well designed benchmarks [203]. In machine learning, a benchmark is composed by a dataset and a task that a model should perform well on, while generalising from training to testing instances from within that dataset. In meta-learning, benchmark design is more complex, since we are often dealing with a learner that should be (meta) trained on a set of tasks, after which it should generalize to learning on previously unseen tasks. Benchmark design is thus more complex due to the need to define families of tasks from which meta-training and meta- testing tasks can be drawn. In this section we will outline the main few-shot benchmarks.

**Benchmarks and Setup** Most FSL studies consider the set-to-set setting, where
a model must learn to do well in a large number of small few-shot learning tasks. Each such task is composed of a small training set (referred to as a support set) consisting of a number of a few labelled examples from a number of classes and a small validation set (referred to as a query set) consisting of previously unseen instances of the same classes contained in the support set. A learner should be able to extract task-specific information from a support set, and then generate a model that can perform well on the query set. Across-task knowledge can be learned by learning the learner that can do this task well. We usually use the notation of N-way K-shot task, to indicate a task with N classes per task, and K samples per class.

There are a number of established FSL datasets that are
used in this setting, such as miniImageNet [41], tieredIm- ageNet [204], SlimageNet [205], CUB-200 [110] and Om- niglot [86]. These benchmarks re-purpose prior datasets with rather large numbers of classes by breaking them into many smaller (lower ‘way’) recognition problems to define a task distribution for benchmarking meta-training and meta- testing.

**Dataset Diversity, Bias and Generalization** While the above approach is convenient to generate enough tasks for training and evaluation, it suffers from a lack of diversity (narrow p(T)) which makes it hard for performance on these benchmarks to reflect performance on real-world few shot task. For example, switching between different kinds of animals in miniImageNet or birds in CUB is a rather weak test of transferability. Ideally we would like to span more diverse categories and types of images (satellite, med- ical, agricultural, underwater, etc); and even be robust to domain-shifts between meta-train and meta-test tasks. There is much work still to be done here as, even
in the many-shot setting, fitting a deep model to a very wide distribution of data is itself non-trivial [206], as is generalising to out-of-sample data [44], [92]. In particular, the performance of meta-learners has been shown to drop drastically when introducing a domain shift between the source and target task distributions [110]. This motivates the recent Meta-Dataset [207] and CVPR cross-domain few- shot challenge [208]. Meta-Dataset aggregates a number of individual recognition benchmarks to provide a wider distribution of tasks p(T) to evaluate the ability to fit a wide task distribution and generalize across domain-shift.
Meanwhile, [208] challenges methods to generalize from the everyday images of ImageNet to medical, satellite and agri- cultural images. Recent work has begun to try and address these issues by meta-training for domain-shift robustness as well as sample efficiency [173]. Generalization issues also arise in applying models to data from under-represented countries [209]. Another recent dataset that could facilitate research in few-shot learner generalization is [210], which offers samples across environments from simulation, to high definition simulation and real-world.

**Real-World Few-Shot Recognition** The most common few-shot problem setting is N-way recognition among the classes in the support set [19], [20]. However, this may not be representative of practical application requirements where recognition among both the source and target is of interest at testing-time. This generalized few-shot setting is considered in an increasing number of studies [128], [165], [211]. In a generalized few-shot setting, other goals include efficient incremental enrolment of novel few-shot classes without forgetting the base classes or re-accessing the source data [128], [165]. Other real-world challenges include scaling up few-shot learning beyond the widely studied N = 1 . . . 20- way recognition setting, at which point the popular and effective metric learner method family [20], [87] begin to struggle.

**Few-Shot Object Detection** The few studies [165] on few- shot detection have thus far re-purposed standard detection datasets such as COCO and Pascal VOC. However these only offer a few classes for meta-training/testing compared to classification benchmarks, and so more benchmarks are needed

**Regression Benchmarks** Unfortunately there has been less work on establishing common benchmarks for few-shot regression than for classification. Toy problems such as 1d sinusoidal regressions have been proposed in [19], [212]. Im- age completion by regressing from pixel coordinate to RGB value have been considered [166], some work regresses to interest points in human pose and fashion [165], while [213] considers the task of face pose regression, with additional occlusion to introduce ambiguity. Overall, these tasks are all scattered and the meta-learning community has yet to reach consensus on regression benchmarks.

**Non meta-learning few-shot methods** Recently, a num- ber of non meta-learning methods have obtained competi- tive performance on few-shot benchmarks, questioning the need for learning to learn in this setting. It was shown in [110] that training on all the base tasks at once and finetun- ing on the target tasks is a stronger baseline than initially reported, mainly because augmentation was unfairly omit- ted. Furthermore, using a deeper backbone may shrink the performance gap between common meta-learning methods, and the baseline can outperform these methods for larger domain shifts between source and target task distributions [207] – although more recent meta-learning methods ob- tained good performance in this setting [173]. On a similar theme, [214] show that simple feature transformations like L2-normalization can make a nearest neighbour classifier competitive without meta-learning. Thus the debate here is ongoing, but overall carefully implemented baselines and more diverse datasets are important, as well as maintaining
fair and consistent best practice for all methods

<!-- REFERENCE -->
