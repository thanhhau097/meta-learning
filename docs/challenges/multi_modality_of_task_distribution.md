# multi_modality_of_task_distribution

Many meta-learning frameworks [19] implicitly assume that the distribution over tasks p(T) is uni-modal, and a single learning strategy Ï‰ pro- vides a good solution for them all. However in reality task distributions can clearly be multi-modal. Consider for ex- ample, medical vs satellite vs everyday images in computer vision. Or the diversity of tasks that robots could be asked to perform from putting pegs in holes to opening doors [230]. Different tasks within the distribution may require different learning strategies, and this can degrade existing meta-learner performance. In vanilla multi-task learning, this phenomenon is relatively well studied with, e.g., meth- ods that group tasks into clusters [286] or subspaces [287]. However this area is only just beginning to be explored in meta-learning [288].

<!-- REFERENCE -->


<details>
<summary>[230] Meta-world: A Benchmark And Evaluation For Multitask And Meta Reinforcement Learning</summary>
<br>
<!-- (meta_world_a_benchmark_and_evaluation_for_multitask_and_meta_reinforcement_learning.md) -->

# meta_world_a_benchmark_and_evaluation_for_multitask_and_meta_reinforcement_learning.md

<!-- REFERENCE -->


[Meta-world: A Benchmark And Evaluation For Multitask And Meta Reinforcement Learning](../papers/meta_world_a_benchmark_and_evaluation_for_multitask_and_meta_reinforcement_learning.md)

</details>



<details>
<summary>[19] Model-Agnostic Meta-learning For Fast Adaptation Of Deep Networks</summary>
<br>
<!-- (model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md) -->

# model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md

<!-- REFERENCE -->


[Model-Agnostic Meta-learning For Fast Adaptation Of Deep Networks](../papers/model_agnostic_meta_learning_for_fast_adaptation_of_deep_networks.md)

</details>



<details>
<summary>[286] Learning With Whom To Share In Multi-task Feature Learning</summary>
<br>
<!-- (learning_with_whom_to_share_in_multi_task_feature_learning.md) -->

# learning_with_whom_to_share_in_multi_task_feature_learning.md

<!-- REFERENCE -->


[Learning With Whom To Share In Multi-task Feature Learning](../papers/learning_with_whom_to_share_in_multi_task_feature_learning.md)

</details>



<details>
<summary>[288] Infinite Mixture Prototypes For Few-shot Learning</summary>
<br>
<!-- (infinite_mixture_prototypes_for_few_shot_learning.md) -->

# infinite_mixture_prototypes_for_few_shot_learning.md

<!-- REFERENCE -->


[Infinite Mixture Prototypes For Few-shot Learning](../papers/infinite_mixture_prototypes_for_few_shot_learning.md)

</details>



<details>
<summary>[287] A Unified Perspective On MultiDomain And Multi-Task Learning</summary>
<br>
<!-- (a_unified_perspective_on_multidomain_and_multi_task_learning.md) -->

# a_unified_perspective_on_multidomain_and_multi_task_learning.md

<!-- REFERENCE -->


[A Unified Perspective On MultiDomain And Multi-Task Learning](../papers/a_unified_perspective_on_multidomain_and_multi_task_learning.md)

</details>

