# systems

**Network Compression**
Contemporary CNNs require
large amounts of memory that may be prohibitive on embedded devices. Thus network compression in various forms such as quantization and pruning are topical research areas [277], [278]. Meta-learning is beginning to be applied to this objective as well, such as training gradient generator meta-networks that allow quantized networks to be trained [187], and weight generator meta-networks that allow quan- tized networks to be trained with gradient [279].

**Communications** Deep learning has recently made waves in communications systems. For example by learning cod- ing systems that exceed the best hand designed codes for realistic channels [280]. Insofar as optimal performance is achieved by learning a coding scheme tuned for the char- acteristics of a particular channel, few-shot meta-learning can be used to provide rapid online adaptation of coding to changing channel characteristics [281].

**Learning with Label Noise** is a challenge in contem- porary deep learning when large datasets are collected by web scraping or crowd-sourcing. Again, while there are several algorithms hand-designed for this situation, recent meta-learning methods have addressed label noise by trans- ductive learning sample-wise weighs to down-weight noisy samples [142], or learning an initial condition robust to noisy label training [93].

**Adversarial Attacks and Defenses** Deep Neural Net- works can be easily fooled into misclassifying a data point that should be easily recognizable, by adding a carefully crafted human-invisible perturbation to the data [282]. Nu- merous methods have been published in recent years in- troducing stronger attack and defense methods. Typical defenses are carefully hand-designed architectures or train- ing strategies. Analogous to the case in domain-shift, an under-studied potential application of meta-learning is to train the learning algorithm end-to-end for robustness by defining a meta-loss in terms of performance under adver- sarial attack [94], [283]. New benchmarks for adversarial defenses have recently been proposed [284] where defenses should generalize to unforseen attacks. It will be interesting to see whether future meta-learning approaches can make progress on this benchmark.
<!-- REFERENCE -->


<details>
<summary>[282] Explaining And Harnessing Adversarial Examples</summary>
<br>
<!-- (explaining_and_harnessing_adversarial_examples.md) -->

# explaining_and_harnessing_adversarial_examples.md

<!-- REFERENCE -->


[Explaining And Harnessing Adversarial Examples](../papers/explaining_and_harnessing_adversarial_examples.md)

</details>



<details>
<summary>[94] Adversarially Robust Few-shot Learning: A Meta-learning Approach</summary>
<br>
<!-- (adversarially_robust_few_shot_learning_a_meta_learning_approach.md) -->

# adversarially_robust_few_shot_learning_a_meta_learning_approach.md

<!-- REFERENCE -->


[Adversarially Robust Few-shot Learning: A Meta-learning Approach](../papers/adversarially_robust_few_shot_learning_a_meta_learning_approach.md)

</details>



<details>
<summary>[279] Metapruning: Meta Learning For Automatic Neural Network Channel Pruning</summary>
<br>
<!-- (metapruning_meta_learning_for_automatic_neural_network_channel_pruning.md) -->

# metapruning_meta_learning_for_automatic_neural_network_channel_pruning.md

<!-- REFERENCE -->


[Metapruning: Meta Learning For Automatic Neural Network Channel Pruning](../papers/metapruning_meta_learning_for_automatic_neural_network_channel_pruning.md)

</details>



<details>
<summary>[278] T-net: Parametrizing Fully Convolutional Nets With A Single Highorder Tensor</summary>
<br>
<!-- (t_net_parametrizing_fully_convolutional_nets_with_a_single_highorder_tensor.md) -->

# t_net_parametrizing_fully_convolutional_nets_with_a_single_highorder_tensor.md

<!-- REFERENCE -->


[T-net: Parametrizing Fully Convolutional Nets With A Single Highorder Tensor](../papers/t_net_parametrizing_fully_convolutional_nets_with_a_single_highorder_tensor.md)

</details>



<details>
<summary>[280] An Introduction To Deep Learning For The Physical Layer</summary>
<br>
<!-- (an_introduction_to_deep_learning_for_the_physical_layer.md) -->

# an_introduction_to_deep_learning_for_the_physical_layer.md

<!-- REFERENCE -->


[An Introduction To Deep Learning For The Physical Layer](../papers/an_introduction_to_deep_learning_for_the_physical_layer.md)

</details>



<details>
<summary>[283] Adversarial Meta-Learning</summary>
<br>
<!-- (adversarial_meta_learning.md) -->

# adversarial_meta_learning.md

<!-- REFERENCE -->


[Adversarial Meta-Learning](../papers/adversarial_meta_learning.md)

</details>



<details>
<summary>[187] MetaQuant: Learning To Quantize By Learning To Penetrate Non-differentiable Quantization</summary>
<br>
<!-- (metaquant_learning_to_quantize_by_learning_to_penetrate_non_differentiable_quantization.md) -->

# metaquant_learning_to_quantize_by_learning_to_penetrate_non_differentiable_quantization.md

<!-- REFERENCE -->


[MetaQuant: Learning To Quantize By Learning To Penetrate Non-differentiable Quantization](../papers/metaquant_learning_to_quantize_by_learning_to_penetrate_non_differentiable_quantization.md)

</details>



<details>
<summary>[284] Testing Robustness Against Unforeseen Adversaries</summary>
<br>
<!-- (testing_robustness_against_unforeseen_adversaries.md) -->

# testing_robustness_against_unforeseen_adversaries.md

<!-- REFERENCE -->


[Testing Robustness Against Unforeseen Adversaries](../papers/testing_robustness_against_unforeseen_adversaries.md)

</details>



<details>
<summary>[93] Learning To Learn From Noisy Labeled Data</summary>
<br>
<!-- (learning_to_learn_from_noisy_labeled_data.md) -->

# learning_to_learn_from_noisy_labeled_data.md

<!-- REFERENCE -->


[Learning To Learn From Noisy Labeled Data](../papers/learning_to_learn_from_noisy_labeled_data.md)

</details>



<details>
<summary>[281] MIND: Model Independent Neural Decoder</summary>
<br>
<!-- (mind_model_independent_neural_decoder.md) -->

# mind_model_independent_neural_decoder.md

<!-- REFERENCE -->


[MIND: Model Independent Neural Decoder](../papers/mind_model_independent_neural_decoder.md)

</details>



<details>
<summary>[277] Compressing Neural Networks Using The Variational Information Bottleneck</summary>
<br>
<!-- (compressing_neural_networks_using_the_variational_information_bottleneck.md) -->

# compressing_neural_networks_using_the_variational_information_bottleneck.md

<!-- REFERENCE -->


[Compressing Neural Networks Using The Variational Information Bottleneck](../papers/compressing_neural_networks_using_the_variational_information_bottleneck.md)

</details>



<details>
<summary>[142] Meta-Weight-Net: Learning An Explicit Mapping For Sample Weighting</summary>
<br>
<!-- (meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md) -->

# meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md

<!-- REFERENCE -->


[Meta-Weight-Net: Learning An Explicit Mapping For Sample Weighting](../papers/meta_weight_net_learning_an_explicit_mapping_for_sample_weighting.md)

</details>

