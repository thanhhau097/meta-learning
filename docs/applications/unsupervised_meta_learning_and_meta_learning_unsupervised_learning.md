# unsupervised_meta_learning_and_meta_learning_unsupervised_learning

In the meta-learning literature, there exist two main variants of meta-learning involving unsupervised learning. In the first one the meta-objective of the outer loop is unsuper- vised, and therefore the learner itself is learned without any labels available. We refer to this case as Unsupervised Meta-Learning. In the second variant, meta-learning is used as a means to learn an unsupervised inner loop task. The outer objective in this case can be anything from supervised, unsupervised or reinforcement based. We refer to this as Meta-Learning Unsupervised Learning. Unsupervised Meta-Learning [247]â€“[249] aims to relax the conventional assumption of an annotated set of source tasks for meta-training, while still producing good downstream performance for supervised few-shot learning. Typically synthetic source tasks are constructed without supervision via clustering or class-preserving data augmentation. Meta-Learning Unsupervised Learning aims to use meta-
learning to train unsupervised learning algorithms that work well for downstream supervised learning tasks. One can train unsupervised clustering algorithms [21], [250], [251] or losses [98], [116] such that downstream supervised learning performance is optimized. This helps to deal with the ill-definedness of the unsupervised learning problem by transforming it into a problem with a clear (meta) super- vised objective.
<!-- REFERENCE -->
